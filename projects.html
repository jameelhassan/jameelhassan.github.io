<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Publications</title>
  
  <meta name="author" content="Jameel Hassan">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="images/favicon.ico">

</head>

<!-- NAVIGATION -->
<div class="header" style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;padding-bottom:0px">
  <!-- <a href="index.html" class="logo" style="color:black;font-size:18px;">Jameel Hassan</a> -->
  <a href="index.html" class="logo" style="color:black;font-size:18px; padding-top:30px"><img src="images/intent_logo.png" height="80"></a>
  <div class="header-right" style="padding-top:50px;">
<!--     <a href="people.html" style="font-size: 23px; color:black">People</a>
    <a href="research.html" style="font-size: 23px; color:black">Research</a>
    <a href="publications.html" style="font-size: 23px; color:black">Publications</a> -->
    <a href="https://github.com/jameelhassan"><i class="fa fa-github" style="font-size:28px;color:black;"></i></a>
    <br>
    
<!--     <div class="header-right" style="padding-top:0px;">
      <a href="https://github.com/CMU-IntentLab"><i class="fa fa-github" style="font-size:28px;color:black;"></i></a>
      <a href="https://www.youtube.com/channel/UCXncV-Q5Iit_PmqABQauptQ"><i class="fa fa-youtube" style="font-size:28px;color:black;"></i></a>
    </div> -->
  </div>
</div>

<!-- CONTENT -->
<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
      <td style="padding:0px">


      <!-- PRE-PRINTS HEADING -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
          <tr>
          <td style="padding-left:20px;padding-bottom:0px;width:100%;vertical-align:middle">
            <heading><b>Projects</b></heading>
          </td>
        </tr>
        </tbody>
      </table>

     <!-- PRE-PRINTS -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
            <img src="paper_imgs/tian_ijrr_2025.gif" style="border-radius:5%/10%;width:200px">
          </td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Maximizing Alignment with Minimal Feedback: Efficiently Learning Rewards for Visuomotor Robot Policy Alignment</papertitle>
              <br>
             R. Tian, Y. Wu, C. Xu, M. Tomizuka, J. Malik, A. Bajcsy
              <br>
              <em>arXiv</em>, 2024
              <br>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2412.04835"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-website" href="https://sites.google.com/berkeley.edu/rapl"><i class="fa fa-external-link"></i> website</a>
              <p></p>
            </td>
          </tr>


          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
            <img src="paper_imgs/yang_agentsim_2024.gif" style="border-radius:5%/10%;width:200px">
          </td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos</papertitle>
              <br>
              G. Yang, A. Bajcsy, S. Saito*, A. Kanazawa* 
              <br>
              <em>arXiv</em>, 2024
              <br>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2410.16259"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-website" href="https://gengshan-y.github.io/agent2sim-www/"><i class="fa fa-external-link"></i> website</a>
              <p></p>
            </td>
          </tr>

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
            <img src="paper_imgs/zhao_conformal_2024.png" style="border-radius:5%/10%;width:200px">
          </td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Conformalized Interactive Imitation Learning: <br> Handling Expert Shift and Intermittent Feedback</papertitle>
              <br>
              M. Zhao, R.Simmons, H.Admoni, A. Ramdas<sup>†</sup>, A. Bajcsy<sup>†</sup>
              <br>
              <em>arXiv</em>, 2024
              <br>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2410.08852"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-website" href="https://cmu-intentlab.github.io/conformalized-interactive-il/"><i class="fa fa-external-link"></i> website</a>
              <p></p>
            </td>
          </tr>

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
            <img src="paper_imgs/santos_li_2024_langSafe.png" style="border-radius:5%/10%;width:200px">
          </td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Updating Robot Safety Representations Online <br> from Natural Language Feedback</papertitle>
              <br>
              L. Santos*, Z. Li*, L. Peters, S. Bansal<sup>†</sup>, A. Bajcsy<sup>†</sup>
              <br>
              <em>arXiv</em>, 2024
              <br>
              <a style="font-size: 12px; color:grey">* equal contribution, † equal advising</a>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2409.14580"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-website" href="https://cmu-intentlab.github.io/language-informed-safe-navigation/"><i class="fa fa-external-link"></i> website</a>
              <p></p>
            </td>
          </tr>

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
            <img src="paper_imgs/pandya_2024.png" style="border-radius:5%/10%;width:200px">
          </td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Robots that Learn to Safely Influence via Prediction-Informed <br> Reach-Avoid Dynamic Games</papertitle>
              <br>
              R. Pandya, C. Liu, A. Bajcsy
              <br>
              <em>arXiv</em>, 2024
              <br>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2409.12153"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-website" href="https://cmu-intentlab.github.io/safe-influence/"><i class="fa fa-external-link"></i> website</a>
              <p></p>
            </td>
          </tr>

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
            <img src="paper_imgs/jeong_2024_v2.png" style="border-radius:5%/10%;width:200px">
          </td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Robots that Suggest Safe Alternatives</papertitle>
              <br>
              H.J. Jeong and A. Bajcsy
              <br>
              <em>arXiv</em>, 2024
              <br>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2409.09883"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-website" href="https://cmu-intentlab.github.io/salt/"><i class="fa fa-external-link"></i> website</a>
              <p></p>
            </td>
          </tr>  


            
          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
            <img src="paper_imgs/bajcsy_2024.png" style="border-radius:5%/10%;width:200px">
          </td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Human–AI Safety: A Descendant of Generative AI and Control Systems Safety</papertitle>
              <br>
              A. Bajcsy and J. Fisac
              <br>
              <em>arXiv</em>, 2024
              <br>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2405.09794"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              <p></p>
            </td>
          </tr>  
          
          

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
            <img src="paper_imgs/li_2024.png" style="border-radius:5%/10%;width:200px">
          </td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Intent Demonstration in General-Sum Dynamic Games <br> via Iterative Linear-Quadratic Approximations</papertitle>
              <br>
              J. Li, A. Siththaranjan, S. Sojoudi, C. Tomlin, A. Bajcsy
              <br>
              <em>arXiv</em>, 2024
              <br>
              <br>
              <a class="button-paper" href="http://arxiv.org/abs/2402.10182"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-code" href="https://github.com/jamesjingqili/Active-Intent-Demonstration-in-Games"><i class="fa fa-code" aria-hidden="true"></i> code</a>
              <p></p>
            </td>
          </tr> 

        </tbody>

     </table>




      <!-- PUBS HEADING -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
            <td style="padding-left:20px;padding-bottom:0px;width:100%;vertical-align:middle">
              <heading><b>Publications</b></heading>
            </td>
          </tr>
          </tbody>
        </table>

      <!-- PUBS -->
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
            <img src="paper_imgs/nakamura_2024_hardware.png" style="border-radius:5%/10%;width:200px">
          </td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Not All Errors Are Made Equal: <br> A Regret Metric for Detecting System-level Trajectory Prediction Failures</papertitle>
              <br>
              K. Nakamura, R. Tian, A. Bajcsy
              <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2024
              <br>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2403.04745"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-website" href="https://cmu-intentlab.github.io/not-all-errors/"><i class="fa fa-external-link"></i> website</a>
              &nbsp
              <a class="button-code" href="https://github.com/CMU-IntentLab/not-all-errors"><i class="fa fa-code" aria-hidden="true"></i> code</a> 
              <p></p>
            </td>
          </tr> 



          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
            <img src="paper_imgs/thakkar_2023.png" style="border-radius:5%/10%;width:200px">
          </td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Adaptive Human Trajectory Prediction via Latent Corridors</papertitle>
              <br>
              N. Thakkar, K. Mangalam, A. Bajcsy, J. Malik
              <br>
              <em>European Conference on Computer Vision</em>, 2024
              <br>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2312.06653"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-website" href="https://neerja.me/atp_latent_corridors/"><i class="fa fa-external-link"></i> website</a>
              <p></p>
            </td>
          </tr> 

        <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
          <img src="paper_imgs/zhao_2024.png" style="border-radius:5%/10%;width:200px">
        </td>
        <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
            <papertitle>Conformalized Teleoperation: Confidently Mapping <br> Human Inputs to High-Dimensional Robot Actions</papertitle>
            <br>
            M. Zhao, R. Simmons, H. Admoni, A. Bajcsy
            <br>
            <em>Robotics: Science and Systems</em>, 2024
            <br>
            <br>
            <a class="button-paper" href="https://arxiv.org/abs/2406.07767"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
            <p></p>
          </td>
        </tr> 

        <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
            <img src="paper_imgs/bajcsy_sigbovik-2024.png" style="border-radius:5%/10%;width:200px">
          </td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>An Optimal Control Approach to Graphic Design</papertitle>
              <br>
              A. Bajcsy
              <br>
              <em>SIGBOVIK</em>, 2024
              <br>
              <br>
              <a class="button-paper" href="./pdf/bajcsy-SIGBOVIK.pdf"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              <p></p>
            </td>
          </tr> 

        <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
            <img src="paper_imgs/lekeufack2024conformal.png" style="border-radius:5%/10%;width:200px">
          </td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Conformal Decision Theory: Safe Autonomous Decisions without Distributions</papertitle>
              <br>
              J. Lekeufack*, A.N. Angelopoulos*, A. Bajcsy*, M.I. Jordan, J. Malik
              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2024
              <br>
              <a style="font-size: 12px; color:grey">* denotes equal contribution</a>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2310.05921"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-website" href="https://conformal-decision.github.io/"><i class="fa fa-external-link"></i> website</a>
              &nbsp
              <a class="button-code" href="https://github.com/Jordylek/conformal-decision"><i class="fa fa-code" aria-hidden="true"></i> code</a>
              <p></p>
            </td>
          </tr> 

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
            <video width="200px" style="border-radius:5%/10%" controls autoplay muted loop>
              <source  src="paper_imgs/bajcsy_vision_2023.mp4" type="video/mp4">
            </video>
          </td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Learning Vision-Based Pursuit-Evasion Robot Policies</papertitle>
              <br>
              A. Bajcsy*, A. Loquercio*, A. Kumar, J. Malik
              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2024
              <br>
              <a style="font-size: 12px; color:grey">* denotes equal contribution</a>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2308.16185"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-website" href="https://abajcsy.github.io/vision-based-pursuit/"><i class="fa fa-external-link"></i> website</a>
              &nbsp
              <a class="button-code" href="https://github.com/abajcsy/vision-based-pursuit/tree/main"><i class="fa fa-code" aria-hidden="true"></i> code</a>
              <p></p>
            </td>
          </tr> 

          
          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
            <img src="paper_imgs/tian_2023_visual.png" style="border-radius:5%/10%;width:200px">
          </td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>What Matters to <i>You</i>? Towards Visual Representation Alignment for Robot Learning</papertitle>
              <br>
              R. Tian, C. Xu, M. Tomizuka, J. Malik, A. Bajcsy
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2024
              <br>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2310.07932"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              <!-- &nbsp -->
              <!-- <a class="button-website" href="https://conformal-decision.github.io/"><i class="fa fa-external-link"></i> website</a> -->
              <p></p>
            </td>
          </tr> 

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/mehta_2023.png" style="border-radius:5%/10%;width:200px"></td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>StROL: Stabilized and Robust Online Learning from Humans</papertitle>
              <br>
              S.A. Mehta, F. Meng, A. Bajcsy, D.P. Losey
              <br>
              <em>Robotics and Automation Letters (RA-L)</em>, 2024
              <br>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2308.09863"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-video" href="https://www.youtube.com/watch?v=uDGpkvJnY8g"><i class="fa fa-video-camera" aria-hidden="true"></i> video</a>
              <p></p>
            </td>
          </tr> 


        <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/peters_2023.gif" style="border-radius:5%/10%;width:200px"></td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Contingency Games for Multi-Agent Interaction</papertitle>
              <br>
              L. Peters, A. Bajcsy, C.Y Chiu, D. Fridovich-Keil, F. Laine, L. Ferranti, J. Alonso-Mora
              <br>
              <em>Robotics and Automation Letters (RA-L)</em>, 2024
              <br>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2304.05483"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-video" href="https://www.youtube.com/watch?v=icvT74C3LXo"><i class="fa fa-video-camera" aria-hidden="true"></i> video</a>
              &nbsp
              <a class="button-website" href="https://lasse-peters.net/pub/contingency-games/"><i class="fa fa-external-link"></i> website</a>
              &nbsp
              <a class="button-code" href="https://github.com/lassepe/peters2024ral-code"><i class="fa fa-code" aria-hidden="true"></i> code</a>
              <p></p>
            </td>
          </tr> 


        <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle">
          <img src="paper_imgs/hu_2023.png" style="border-radius:5%/10%;width:200px">
        </td>
        <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
            <papertitle>Deception Game: Closing the Safety-Learning Loop in Interactive Robot Autonomy</papertitle>
            <br>
            H. Hu*, Z. Zhang*, K. Nakamura, A. Bajcsy, J.F. Fisac
            <br>
            <em>Conference on Robot Learning (CoRL)</em>, 2023
            <br>
            <a style="font-size: 12px; color:grey">* denotes equal contribution</a>
            <br>
            <a class="button-paper" href="https://arxiv.org/abs/2309.01267"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
            &nbsp
            <a class="button-website" href="https://saferoboticslab.github.io/Belief-Game/"><i class="fa fa-external-link"></i> website</a>
            &nbsp
            <a class="button-code" href="https://github.com/SafeRoboticsLab/Belief-Game/"><i class="fa fa-code" aria-hidden="true"></i> code</a>
            <p></p>
          </td>
        </tr> 

        <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/tian_2023.png" style="border-radius:5%/10%;width:200px"></td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Towards Modeling and Influencing the Dynamics of Human Learning</papertitle>
              <br>
              R. Tian, M. Tomizuka, A. Dragan, A. Bajcsy
              <br>
              <em>International Conference on Human-Robot Interaction (HRI)</em>, 2023
              <br>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2301.00901"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-video" href="https://www.youtube.com/watch?v=KGsVm0qXDAc"><i class="fa fa-video-camera" aria-hidden="true"></i> video</a>
              &nbsp
              <a class="button-website" href="https://sites.google.com/berkeley.edu/midle"><i class="fa fa-external-link" aria-hidden="true"></i> website</a>
              <p></p>
            </td>
          </tr> 

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/shahab_2023.png" style="border-radius:5%/10%;width:200px"></td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Towards Robots that Influence Humans over Long-Term Interaction</papertitle>
              <br>
              S. Sagheb, Y. Mun, N. Ahmadian, B.A. Christie, A. Bajcsy, K. Driggs-Campbell, D.P. Losey
              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2023
              <br>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2209.10588"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-video" href="https://www.youtube.com/watch?v=ydO83cgjZ2Q"><i class="fa fa-video-camera" aria-hidden="true"></i> video</a>
              <p></p>
            </td>
          </tr> 

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/leung_2022.png" style="border-radius:5%/10%;width:200px"></td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Towards the Unification and Data-Driven Synthesis of Autonomous Vehicle Safety Concepts</papertitle>
              <br>
              K. Leung*, A. Bajcsy*, E. Schmerling, M. Pavone
              <br>
              <em>arXiv</em>, 2022
              <br>
              <a style="font-size: 12px; color:grey">* denotes equal contribution</a>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2107.14412"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              <p></p>
            </td>
          </tr> 


          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/tian_2022.gif" style="border-radius:5%/10%;width:200px"></td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Safety Assurances for Human-Robot Interaction via Confidence-aware Game-theoretic Human Models</papertitle>
              <br>
              R. Tian*, L. Sun*, A. Bajcsy*, M. Tomizuka, A.D. Dragan
              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2022
              <br>
              <a style="font-size: 12px; color:grey">* denotes equal contribution</a>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2109.14700"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-code" href="https://github.com/thomasrantian/Confidence-aware-game-theoretic-BRT"><i class="fa fa-code" aria-hidden="true"></i> code</a>
              &nbsp
              <a class="button-talk" href="https://www.youtube.com/watch?v=YZlwMxepGtc&list=PLjbUVJgrbvfnyEXcYwEsTTLnsHu9_cuJa&index=5"><i class="fa fa-volume-up" aria-hidden="true"></i> talk</a>
              <p></p>
            </td>
          </tr> 

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/losey_2021.png" style="border-radius:5%/10%;width:200px"></td>
            <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Physical Interaction as Communication: Learning Robot Objectives Online from Human Corrections</papertitle>
              <br>
              D.P. Losey, A. Bajcsy, M.K. O'Malley, A.D. Dragan
              <br>
              <em>International Journal of Robotics Research (IJRR)</em>, 2021
              <br>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2107.02349"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              <p></p>
            </td>
          </tr> 

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/bajcsy_2021.png" style="border-radius:5%/10%;width:200px"></td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Analyzing Human Models that Adapt Online</papertitle>
              <br>
              A. Bajcsy, A. Siththaranjan, C.J. Tomlin, A.D. Dragan
              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2021
              <br>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2103.05746"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-code" href="https://github.com/abajcsy/pred_analyzer"><i class="fa fa-code" aria-hidden="true"></i> code</a>
              <p></p>
            </td>
          </tr> 

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/ratner_2021.png" style="border-radius:5%/10%;width:200px"></td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Efficient Dynamics Estimation with Adaptive Model Sets</papertitle>
              <br>
              E. Ratner, A. Bajcsy, C.J. Tomlin, A.D. Dragan
              <br>
              <em>IEEE Robotics and Automation Letters (RA-L)</em>, 2021
              <br>
              <br>
              <a class="button-paper" href="https://ieeexplore.ieee.org/document/9357896"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              <p></p>
            </td>
          </tr> 

        <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/turtlebot_human.gif" style="border-radius:5%/10%;width:200px"></td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>A Robust Control Framework for Human Motion Prediction</papertitle>
              <br>
              A. Bajcsy, S. Bansal, E. Ratner, C.J. Tomlin, A.D. Dragan
              <br>
              <em>IEEE Robotics and Automation Letters (RA-L)</em>, 2020
              <br>
              <br>
              <a class="button-paper" href="https://ieeexplore.ieee.org/abstract/document/9210199"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-code" href="https://github.com/abajcsy/hallucinate"><i class="fa fa-code" aria-hidden="true"></i> code</a>
              <p></p>
            </td>
          </tr> 

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/bobu_2020.png" style="border-radius:5%/10%;width:200px"></td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Quantifying Hypothesis Space Misspecification in Learning from Human-Robot Demonstrations and Physical Corrections</papertitle>
              <br>
              A. Bobu, A. Bajcsy, J.F. Fisac, S. Deglurkar, A.D. Dragan
              <br>
              <em>IEEE Transactions on Robotics (T-RO)</em>, 2020 
              <br><p style="display:inline;color:#f18800;">(Honorable Mention for the 2020 IEEE T-RO Best Paper Award)</p>
              <br>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/2002.00941"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-code" href="https://github.com/andreea7b/jaco_learning"><i class="fa fa-code" aria-hidden="true"></i> code</a>
              <p></p>
            </td>
          </tr> 

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/bansal_2020.png" style="border-radius:5%/10%;width:200px"></td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>A Hamilton-Jacobi Reachability-Based Framework for Predicting and Analyzing Human Motion for Safe Planning</papertitle>
              <br>
              S. Bansal*, A. Bajcsy*, E. Ratner*, A.D. Dragan, C.J. Tomlin
              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2020
              <br>
              <a style="font-size: 12px; color:grey">* denotes equal contribution</a>
              <br>
              <a class="button-paper" href="http://arxiv.org/abs/1910.13369"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-video" href="https://www.youtube.com/watch?v=uZi-zIi1S6A"><i class="fa fa-video-camera" aria-hidden="true"></i> video</a>
              <p></p>
            </td>
          </tr> 

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/keil_2019.png" style="border-radius:5%/10%;width:200px"></td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Confidence-aware motion prediction for real-time collision avoidance</papertitle>
              <br>
              D. Fridovich-Keil*, A. Bajcsy*, J.F. Fisac, S.L. Herbert, S. Wang, A.D. Dragan, C.J. Tomlin
              <br>
              <em>International Journal of Robotics Research (IJRR)</em>, 2019
              <br>
              <a style="font-size: 12px; color:grey">* denotes equal contribution</a>
              <br>
              <a class="button-paper" href="https://journals.sagepub.com/doi/full/10.1177/0278364919859436"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-code" href="https://github.com/abajcsy/crazyflie_human/tree/abajcsy/dubins_car"><i class="fa fa-code" aria-hidden="true"></i> code</a>
              <p></p>
            </td>
          </tr> 

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/bajcsy_2019.gif" style="border-radius:5%/10%;width:200px"></td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>An Efficient Reachability-Based Framework for Provably Safe Autonomous Navigation in Unknown Environments</papertitle>
              <br>
              A. Bajcsy*, S. Bansal*, E. Bronstein, V. Tolani, C.J. Tomlin
              <br>
              <em>IEEE Conference on Decision and Control (CDC)</em>, 2019
              <br>
              <a style="font-size: 12px; color:grey">* denotes equal contribution</a>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/1905.00532"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-video" href="https://www.youtube.com/watch?v=BAm3DJFwHis&feature=emb_title"><i class="fa fa-video-camera" aria-hidden="true"></i> video</a>
              &nbsp
              <a class="button-website" href="https://smlbansal.github.io/website-safe-navigation/"><i class="fa fa-external-link" aria-hidden="true"></i> website</a>
              &nbsp
              <a class="button-code" href="https://github.com/abajcsy/safe_navigation/tree/experimental/planning"><i class="fa fa-code" aria-hidden="true"></i> code</a>
              <p></p>
            </td>
          </tr> 

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/multi_human.gif" style="border-radius:5%/10%;width:200px"></td>
            <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>A Scalable Framework For Real-Time Multi-Robot, Multi-Human Collision Avoidance</papertitle>
              <br>
              A. Bajcsy*, S.L. Herbert*, D. Fridovich-Keil, J.F. Fisac, S. Deglurkar, A.D. Dragan, C.J. Tomlin
              <br>
              <em>International Conference on Robotics and Automation (ICRA)</em>, 2019
              <br>
              <a style="font-size: 12px; color:grey">* denotes equal contribution</a>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/1811.05929"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-video" href="https://www.youtube.com/watch?v=lJGRHNJ1_Wk"><i class="fa fa-video-camera" aria-hidden="true"></i> video</a>
              &nbsp
              <a class="button-code" href="https://github.com/HJReachability/faSTPeople"><i class="fa fa-code" aria-hidden="true"></i> code</a>
              <!-- <a href="https://arxiv.org/abs/1811.05929">paper</a> &nbsp&nbsp|&nbsp&nbsp
              <a href="https://www.youtube.com/watch?v=lJGRHNJ1_Wk">video</a> &nbsp&nbsp|&nbsp&nbsp
              <a href="https://github.com/HJReachability/faSTPeople">code</a> -->
              <p></p>
            </td>
          </tr> 

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/bobu_2018.gif" style="border-radius:5%/10%;width:200px"></td>
            <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Learning under Misspecified Objective Spaces</papertitle>
              <br>
              A. Bobu, A. Bajcsy, J.F. Fisac, A.D. Dragan
              <br>
              <em>Conference on Robot Learning (CoRL)</em>, 2018 
              <br>
              <p style="display:inline;color:#f18800;">(invited to special issue)</p>
              <br>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/1810.05157"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-video" href="https://www.youtube.com/watch?v=stnFye8HdcU&feature=youtu.be"><i class="fa fa-video-camera" aria-hidden="true"></i> video</a>
              &nbsp
              <a class="button-code" href="https://github.com/andreea7b/beta_adaptive_pHRI"><i class="fa fa-code" aria-hidden="true"></i> code</a>
              <!-- <a href="https://arxiv.org/abs/1810.05157">paper</a> &nbsp&nbsp|&nbsp&nbsp
              <a href="https://www.youtube.com/watch?v=stnFye8HdcU&feature=youtu.be">video</a> &nbsp&nbsp|&nbsp&nbsp
              <a href="https://github.com/andreea7b/beta_adaptive_pHRI">code</a> -->
              <p></p>
            </td>
          </tr> 

          <br>

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/quadcopter_human.gif" style="border-radius:5%/10%;width:200px"></td>
            <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Probabilistically Safe Robot Planning with Confidence-Based Human Predictions</papertitle>
              <br>
              J.F. Fisac*, A. Bajcsy*, S.L. Herbert, D. Fridovich-Keil, S. Wang, C.J. Tomlin, A.D. Dragan
              <br>
              <em>Robotics: Science and Systems (RSS)</em>, 2018 
              <br>
              <p style="display:inline;color:#f18800;">(invited to special issue)</p>
              <br>
              <a style="font-size: 12px; color:grey">* denotes equal contribution</a>
              <br>
              <a class="button-paper" href="https://arxiv.org/abs/1806.00109"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-video" href="https://www.youtube.com/watch?v=2ZRGxWknENg"><i class="fa fa-video-camera" aria-hidden="true"></i> video</a>
              &nbsp
              <a class="button-code" href="https://github.com/abajcsy/crazyflie_human"><i class="fa fa-code" aria-hidden="true"></i> code</a>
              <p></p>
            </td>
          </tr> 

          <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle;"><img src="paper_imgs/bajcsy_2018.gif" style="border-radius:5%/10%;width:200px"></td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
              <papertitle>Learning from Physical Human Corrections, One Feature at a Time</papertitle>
              <br>
              A. Bajcsy, D.P. Losey,  M.K. O'Malley, A.D. Dragan
              <br>
              <em>International Conference on Human-Robot Interaction (HRI)</em>, 2018
              <br>
              <br>
              <a class="button-paper" href="https://dl.acm.org/citation.cfm?id=3171267"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
              &nbsp
              <a class="button-video" href="https://www.youtube.com/watch?v=JHwDwieAaeU"><i class="fa fa-video-camera" aria-hidden="true"></i> video</a>
              &nbsp
              <a class="button-code" href="https://github.com/abajcsy/iact_control/tree/icra2017"><i class="fa fa-code" aria-hidden="true"></i> code</a>
              <p></p>
            </td>
          </tr> 

        <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle;"><img src="paper_imgs/bajcsy_2017.gif" style="border-radius:5%/10%;width:200px"></td>
        <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
            <papertitle>Learning Robot Objectives from Physical Human Robot Interaction</papertitle>
            <br>
            A. Bajcsy*, D.P. Losey*,  M.K. O'Malley, A.D. Dragan
            <br>
            <em>Conference on Robot Learning (CoRL)</em>, 2017 
            <br>
            <p style="display:inline;color:#f18800;">(oral, acceptance rate 10%)</p>
            <br>
            <a style="font-size: 12px; color:grey">* denotes equal contribution</a>
            <br>
            <a class="button-paper" href="http://proceedings.mlr.press/v78/bajcsy17a/bajcsy17a.pdf"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
            &nbsp
            <a class="button-video" href="https://www.youtube.com/watch?v=1MkI6DH1mcw"><i class="fa fa-video-camera" aria-hidden="true"></i> video</a>
            &nbsp
            <a class="button-talk" href="https://www.youtube.com/watch?v=9xGBaM6ZlOE"><i class="fa fa-volume-up" aria-hidden="true"></i> talk</a>
            &nbsp
            <a class="button-blog" href="https://bair.berkeley.edu/blog/2018/02/06/phri/"><i class="fa fa-rss" aria-hidden="true"></i> blog</a>
            <p></p>
          </td>
        </tr>

        <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/bateman_2017.png" style="border-radius:5%/10%;width:200px"></td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
            <papertitle>A User-Centered Design and Analysis of an Electrostatic Haptic Touchscreen System for Students with Visual Impairments</papertitle>
            <br>
            A. Bateman, O. Zhao, A. Bajcsy, M. Jennings, B. Toth, A. Cohen, E. Horton, A. Khattar, R. Kuo, F. Lee, M.K. Lim, L. Migasiuk, R. Renganathan, A. Zhang, M.A. Oliveira
            <br>
            <em>International Journal of Human-Computer Studies</em>, 2017
            <br>
            <br>
            <a class="button-paper" href="https://www.sciencedirect.com/science/article/pii/S1071581917301301"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
            <p></p>
          </td>
          </tr>

        <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/horton_2016.png" style="border-radius:5%/10%;width:200px"></td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
            <papertitle>A review of principles in design and usability testing of tactile technology for individuals with visual impairments</papertitle>
            <br>
            E.L. Horton, R. Renganathan, B.N. Toth, A.J. Cohen, A. Bajcsy, A. Bateman, M.C. Jennings, A. Khattar, R.S. Kuo, F.A. Lee, M.K. Lim, L.W, Migasiuk, A. Zhang, O.K. Zhao, M.A. Oliveira
            <br>
            <em>Assistive Technology</em>, 2016
            <br>
            <br>
            <a class="button-paper" href="http://tandfonline.com/doi/abs/10.1080/10400435.2016.1176083?scroll=top&needAccess=true&journalCode=uaty20"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
            <p></p>
          </td>
        </tr>


        <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/bajcsy_2015.png" style="border-radius:5%/10%;width:200px"></td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
            <papertitle>Systematic measurement of marginal mark types on voting ballots</papertitle>
            <br>
            A. Bajcsy, Y.S. Li-Baboud, M. Brady
            <br>
            <em>NIST IR 8069</em>, 2015
            <br>
            <br>
            <a class="button-paper" href="http://nvlpubs.nist.gov/nistpubs/ir/2015/NIST.IR.8069.pdf"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
            <p></p>
          </td>
          </tr> 

        <td style="padding-left:20px;padding-bottom:20px;width:15%;vertical-align:middle"><img src="paper_imgs/bajcsy_2013.png" style="border-radius:5%/10%;width:200px"></td>
          <td style="padding-left:35px;padding-bottom:20px;width:100%;vertical-align:left">
            <papertitle>Depicting Web images for the blind and visually impaired</papertitle>
            <br>
            A. Bajcsy, Y.S. Li-Baboud, M. Brady
            <br>
            <em>SPIE Newsroom</em>, 2013
            <br>
            <br>
            <a class="button-paper" href="http://spie.org/x104896.xml"><i class="fa fa-file-text" aria-hidden="true"></i> paper</a>
            <p></p>
          </td>
          </tr>

        </tbody>

</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="0" style="padding-top:0px;">
  <tbody>
    <tr>
      <td>
        <br>
        <!-- <hr> -->
        <p></p>
        <br><br>
      </td>
    </tr>
  </tbody>
</table>

</body>

</html>
